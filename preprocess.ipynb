{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pa05toDnjzl4"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as cp\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import pydub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import msaf\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import IPython.display # IPython.display for audio output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NZ2o_5xllLII"
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'path' : './datasets/karaoke_ky',\n",
    "    'sr' : 44100,\n",
    "    \"n_fft\" : 2048,\n",
    "    \"hop_length\" : 512,\n",
    "    \"msaf_feature\" : 'mfcc',\n",
    "    'msaf_algorithm' : 'cnmf',\n",
    "    'num_segment' : 63,\n",
    "    'stft_center' : True,\n",
    "    'masking':{\n",
    "        'Segment': {\n",
    "            'ratio' : 0.15,\n",
    "        },\n",
    "        'Frequency' : {\n",
    "            'ratio' : 0.15,\n",
    "            'count' : 3,\n",
    "        },\n",
    "    },\n",
    "    'n_mels' : 128,\n",
    "    'batch_size' : 16,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ocAyLqqRrUWN"
   },
   "outputs": [],
   "source": [
    "# plot wave \n",
    "def plot_wave(y):\n",
    "    x = np.arange(0, len(y), 1)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.show()\n",
    "\n",
    "# plot spectrogram\n",
    "def plot_spectrogram(s):\n",
    "    librosa.display.specshow(librosa.amplitude_to_db(s, ref=np.max), y_axis='linear', x_axis='time')\n",
    "    plt.title('power Spectrogram')\n",
    "    plt.colorbar(format=\"%+2.0f db\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qafp6lEllLWJ"
   },
   "outputs": [],
   "source": [
    "def merge_state(state0, state1):\n",
    "    result = {}\n",
    "    for k, v in state0.items():\n",
    "        result[k] = v\n",
    "    for k, v in state1.items():\n",
    "        if k in result.keys():\n",
    "            result[k] += v\n",
    "        else:\n",
    "            result[k] = v\n",
    "    return result\n",
    "\n",
    "def msaf_postprocessing(boundaries, labels, desired_num_boundaries):\n",
    "    \"\"\"\n",
    "    boundaries : <numpy.array> boundary detection result of msaf.process() \n",
    "    labels : <list> segmentation grouping result of msaf.process()\n",
    "    desired_num_boundaries : <int> desired number of boundaries after postprocessing\n",
    "    \"\"\"\n",
    "    \n",
    "    while desired_num_boundaries > len(boundaries):\n",
    "        diff = [ j - i for i, j in zip(boundaries[:-1],boundaries[1:])]\n",
    "        max_idx = np.argmax(diff)\n",
    "#         boundaries.append((boundaries[max_idx] + boundaries[max_idx+1])/2)\n",
    "        boundaries = np.insert(boundaries, max_idx+1 , (boundaries[max_idx] + boundaries[max_idx+1])/2)\n",
    "        boundaries = np.array(sorted(boundaries))\n",
    "        labels.insert(max_idx, labels[max_idx])\n",
    "        \n",
    "    if desired_num_boundaries == len(boundaries):\n",
    "        return boundaries, labels, len(labels) \n",
    "\n",
    "    while desired_num_boundaries < len(boundaries):\n",
    "        # initialize helper arrays(boundaries_difference, average bi-directional boundaries difference, label and diff state)\n",
    "        diff = [ j - i for i, j in zip(boundaries[:-1],boundaries[1:])]\n",
    "        avg_bi_diff = (np.array(diff + [diff[-1]]) + np.array([diff[0]]+diff))/2\n",
    "        state = [{i : j} for i, j in zip(labels, diff)]\n",
    "\n",
    "        removal_idx = np.argmin(avg_bi_diff)\n",
    "\n",
    "        # shift removal idx because can't remove first & last boundary\n",
    "        if removal_idx == 0:\n",
    "            removal_idx += 1\n",
    "        if removal_idx == len(boundaries)-1:\n",
    "            removal_idx -= 1\n",
    "\n",
    "        # update list and state according to minimum index of average bi-directional diff\n",
    "        boundaries = np.delete(boundaries, removal_idx, 0)\n",
    "        avg_bi_diff = np.delete(avg_bi_diff, removal_idx, 0)\n",
    "        state[removal_idx-1: removal_idx+1] = [merge_state(state[removal_idx-1], state[removal_idx])]\n",
    "\n",
    "    # Straighten out state to labels\n",
    "    labels = [max(x, key=x.get)  for x in state]\n",
    "    \n",
    "    return boundaries , labels, len(labels)\n",
    "\n",
    "def time_synchronize(boundaries, random_wave_crop_idx, sr, wave_length):\n",
    "    boundaries = boundaries - random_wave_crop_idx / sr\n",
    "    boundaries[0] = 0\n",
    "    boundaries[-1] = wave_length / sr\n",
    "    return boundaries\n",
    "    \n",
    "def load(file_path, config):\n",
    "    y, sr = librosa.load(file_path, sr =  config['sr'])\n",
    "    random_wave_crop_idx = random.randint(0, len(y)%config['hop_length'])\n",
    "    wave = y[random_wave_crop_idx : random_wave_crop_idx + len(y) - len(y)%config['hop_length']]\n",
    "    spectrogram = librosa.stft(wave, n_fft = config['n_fft'], hop_length = config['hop_length'], center=config['stft_center'])\n",
    "    # if center is True, then w(shape = hop_length * k) result D.shape = [k+1, n_fft/2 + 1].\n",
    "    # if center is False, then w result D.shape = [(k - n_fft/hop_length) + 1, n_fft/2 + 1].\n",
    "    mel_spectrogram = mel_filterbank(spectrogram, sr=config['sr'], n_fft=config['n_fft'], n_mels = config['n_mels'])\n",
    "    return wave, spectrogram, mel_spectrogram, random_wave_crop_idx, len(y)\n",
    "\n",
    "def msaf_process(file_path, config, crop_idx, origin_wave_length):\n",
    "    boundaries, labels = msaf.process(file_path, feature=config['msaf_feature'], \n",
    "                                      boundaries_id=config['msaf_algorithm'], labels_id=config['msaf_algorithm'])\n",
    "    boundaries, labels, seg_num = msaf_postprocessing(boundaries, labels.copy(), config['num_segment']+1)\n",
    "    boundaries = time_synchronize(boundaries, crop_idx, config['sr'], origin_wave_length)\n",
    "    return boundaries, labels, seg_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "53FEsAdslLhJ"
   },
   "outputs": [],
   "source": [
    "# masking\n",
    "def mel_filterbank(spectrogram, sr, n_fft, n_mels):\n",
    "    mel_filter = librosa.filters.mel(sr = sr , n_fft = n_fft, n_mels = n_mels)\n",
    "    return (mel_filter @ np.abs(spectrogram)**2)\n",
    "\n",
    "def masking_spectrogram(spectrogram, config, boundaries, labels):\n",
    "    \"\"\"\n",
    "    mode : choice of masking mode in ['Segment', 'ConsecutiveFrame', 'Frequency', 'Change', ...] \n",
    "\n",
    "    \"\"\"\n",
    "    mode = config['masking']\n",
    "    masked_spectrogram = spectrogram.copy()\n",
    "    masking_index = []\n",
    "\n",
    "    if 'Segment' in mode.keys():\n",
    "        seg_masking_ratio = mode['Segment']['ratio']\n",
    "        masking_labels = random.sample(population = set(labels), k= int(seg_masking_ratio * len(set(labels))))\n",
    "        for masking_label in masking_labels:\n",
    "            for masking_idx in np.argwhere(np.array(labels)==masking_label).flatten():\n",
    "                start, end = boundaries[masking_idx], boundaries[masking_idx+1]\n",
    "                start = math.floor(start*config['sr']/config['hop_length'])\n",
    "                end = math.ceil(end*config['sr']/config['hop_length'])\n",
    "                masked_spectrogram[:, start : end] = 0\n",
    "                masking_index.append([start,end])\n",
    "        \n",
    "    if 'ConsecutiveFrame' in mode.keys():\n",
    "        cons_frame_masking_ratio, cons_frame_masking_count = mode['ConsecutiveFrame']['ratio'], mode['ConsecutiveFrame']['count']\n",
    "        frames = spectrogram.shape[1]\n",
    "        masking_range = int(frames * cons_frame_masking_ratio)\n",
    "        for _ in range(cons_frame_masking_count):\n",
    "            masked_idx = random.randint(0,int(frames-masking_range))\n",
    "            masking_index.append([masked_idx, masking_range])\n",
    "            masked_spectrogram[:,masked_idx:masked_idx + masking_range] = 0\n",
    "            masking_index.append([masked_idx , masked_idx + masking_range])\n",
    "\n",
    "    if 'Frequency' in mode.keys():\n",
    "        freq_masking_ratio, freq_masking_count = mode['Frequency']['ratio'], mode['Frequency']['count']\n",
    "        freqs = spectrogram.shape[0]\n",
    "        masking_range = int(freqs * freq_masking_ratio)\n",
    "        for _ in range(freq_masking_count):\n",
    "            masked_freq = random.randint(0, int(freqs - masking_range))\n",
    "            masked_spectrogram[masked_freq:masked_freq+masking_range] = 0\n",
    "\n",
    "    return masked_spectrogram, masking_index\n",
    "\n",
    "def masking(spectrogram, config, boundaries, labels):\n",
    "    masked_spectrogram, masking_index = masking_spectrogram(spectrogram, config, boundaries, labels)\n",
    "    masked_mel_spectrogram = mel_filterbank(masked_spectrogram, sr=config['sr'], n_fft=config['n_fft'], n_mels = config['n_mels'])\n",
    "    masked_wave = librosa.istft(masked_spectrogram, hop_length = config['hop_length'], win_length = config['n_fft'])\n",
    "    return masked_wave, masked_mel_spectrogram, masking_index            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UXGPAHBHezq0"
   },
   "outputs": [],
   "source": [
    "def store_mp3(f, x, sr=44100, normalized=False):\n",
    "    \"\"\"\n",
    "    numpy array to MP3\n",
    "    reference : https://stackoverflow.com/questions/53633177/how-to-read-a-mp3-audio-file-into-a-numpy-array-save-a-numpy-array-to-mp3\n",
    "    \"\"\"\n",
    "    if normalized:  # normalized array - each item should be a float in [-1, 1)\n",
    "        y = np.int16(x * 2 ** 15)\n",
    "    else:\n",
    "        y = np.int16(x)\n",
    "    song = pydub.AudioSegment(y.tobytes(), frame_rate=sr, sample_width=2, channels=1)\n",
    "    song.export(f+\".mp3\", format=\"mp3\", bitrate=\"320k\")\n",
    "\n",
    "def store_obj(f, obj):\n",
    "    with open(f, 'wb') as file:\n",
    "        pickle.dump(obj, file)\n",
    "\n",
    "def filename_hashing(filename):\n",
    "    hash_object = hashlib.sha1(bytes(filename, encoding='cp949'))\n",
    "    return str(hash_object.hexdigest())\n",
    "\n",
    "\n",
    "def preprocess(config, features, algorithms,  exts=['mp3', 'wav']):\n",
    "\n",
    "    dataset_path = os.path.join(config['path'], 'audio')\n",
    "    preprocessed_path = os.path.join(config['path'], 'preprocessed',  \"num_seg_\"+str(config['num_segment']+1) )\n",
    "\n",
    "    if not os.path.isdir(dataset_path):\n",
    "        raise Exception(\"dataset path is invalid!\")\n",
    "    if not os.path.isdir(preprocessed_path):\n",
    "        os.mkdir(preprocessed_path)\n",
    "    if not os.path.isdir(preprocessed_path):\n",
    "        os.mkdir(preprocessed_path)\n",
    "\n",
    "    # boundary_algorithms = msaf.get_all_boundary_algorithms()\n",
    "    # label_algorithms = msaf.get_all_label_algorithms()\n",
    "    # algorithms = set(boundary_algorithms) & set(label_algorithms)\n",
    "    # features = list(msaf.features_registry.keys())\n",
    "\n",
    "    meta_result = []\n",
    "    error_feature_algorithm_set = set()\n",
    "    for filename in os.listdir(dataset_path):\n",
    "        if filename.split(\".\")[-1].lower() in exts: # check file extension\n",
    "            hashed_filename = filename_hashing(\".\".join(filename.split(\".\")[:-1]))\n",
    "\n",
    "            # crop with hop_length multiple & Short Time Fourier Transform for spectrogram\n",
    "            wave, spectrogram, mel_spectrogram, crop_idx, origin_wave_length = load(os.path.join(dataset_path, filename), config )\n",
    "\n",
    "            # store cropped wave(mp3) & spectrogram(pickle)\n",
    "            cropped_wave_file_name = os.path.join(preprocessed_path, \"cropped_\" + hashed_filename)\n",
    "            melspectrogram_file_name = os.path.join(preprocessed_path, 'melspectrogram_'+hashed_filename)  \n",
    "            store_mp3(cropped_wave_file_name, wave, config['sr'], normalized=True)\n",
    "            store_obj(melspectrogram_file_name, mel_spectrogram)\n",
    "\n",
    "            # meta information buffer for total meta.csv\n",
    "            meta_buffer = [os.path.join(dataset_path, filename), hashed_filename, cropped_wave_file_name+\".mp3\", melspectrogram_file_name]\n",
    "\n",
    "            for feature in features:\n",
    "                for algorithm in algorithms:\n",
    "                    try:\n",
    "                        config['msaf_feature'] = feature\n",
    "                        config['msaf_algorithm'] = algorithm\n",
    "                        \n",
    "                        # msaf & masking\n",
    "                        boundaries, labels, seg_num = msaf_process(os.path.join(dataset_path, filename), config, crop_idx, origin_wave_length)\n",
    "                        masked_wave, masked_mel_spectrogram, masking_index = masking(spectrogram, config, boundaries, labels)\n",
    "                        \n",
    "                        # check between origin data shape and return of masking process shape\n",
    "                        assert wave.shape == masked_wave.shape\n",
    "                        assert mel_spectrogram.shape == masked_mel_spectrogram.shape\n",
    "                        \n",
    "                        # store masked wave(mp3) & others(pickle)\n",
    "                        masked_wave_file_name = os.path.join(preprocessed_path,  \"masked_{}_{}_\".format(feature, algorithm) + hashed_filename)\n",
    "                        others_file_name = os.path.join(preprocessed_path, \"others_{}_{}_\".format(feature,algorithm)+hashed_filename)\n",
    "                        store_mp3(masked_wave_file_name, masked_wave, config['sr'],normalized=True)\n",
    "                        store_obj(others_file_name, [masked_mel_spectrogram, boundaries, labels, seg_num, masking_index])\n",
    "                        meta_buffer.append(masked_wave_file_name+'.mp3')\n",
    "                        meta_buffer.append(others_file_name)\n",
    "                    except Exception as ex:\n",
    "                        # TODO : error handling, logging\n",
    "                        error_feature_algorithm_set.add(\"{}_{}\".format(feature, algorithm))\n",
    "                        print(feature, algorithm, ex)\n",
    "\n",
    "            # append buffer of meta info in result \n",
    "            meta_result.append(meta_buffer)\n",
    "    # store result to csv\n",
    "    try:\n",
    "        columns = \"origin_file_path,hashed_filename,cropped_file_path,mel_file_path,\"\n",
    "        for feature in features:\n",
    "            for algorithm in algorithms:\n",
    "                if \"{}_{}\".format(feature, algorithm) not in error_feature_algorithm_set:\n",
    "                    columns += \"masked_wave_{}_{},\".format(feature, algorithm)\n",
    "                    columns += \"masked_other_{}_{},\".format(feature, algorithm)\n",
    "        np.savetxt(os.path.join(preprocessed_path, \"meta.csv\"), np.array(meta_result), delimiter=\",\", fmt=\"%s\", header=columns[:-1])\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "    return meta_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 145
    },
    "colab_type": "code",
    "id": "LBRtPGIc8oXJ",
    "outputId": "5961e9f6-193c-4478-9467-7880847ad438"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/librosa/beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n",
      "/usr/local/lib/python3.6/dist-packages/librosa/beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n",
      "/usr/local/lib/python3.6/dist-packages/librosa/beat.py:306: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
      "  hop_length=hop_length))\n"
     ]
    }
   ],
   "source": [
    "mr = preprocess(config, ['mfcc'], ['cnmf'])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "preprocess.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
